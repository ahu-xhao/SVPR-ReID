from torch.utils.data.sampler import Sampler
from collections import defaultdict
import copy
import random
import numpy as np

from typing import List, Optional
from collections import defaultdict
import copy
import itertools
import math
from utils import comm


def no_index(a, b):
    assert isinstance(a, list)
    return [i for i, j in enumerate(a) if j != b]


def same_index(a, b):
    assert isinstance(a, list)
    return [i for i, j in enumerate(a) if j == b]


def view_divide(s_index, template):

    assert isinstance(s_index, list)
    same_view_idx = []
    diff_view_idx = []
    for i, j in enumerate(s_index):
        if j == template:
            same_view_idx.append(i)
        else:
            diff_view_idx.append(i)
    return same_view_idx, diff_view_idx


def reorder_index(batch_indices, world_size):
    r"""Reorder indices of samples to align with DataParallel training.
    In this order, each process will contain all images for one ID, triplet loss
    can be computed within each process, and BatchNorm will get a stable result.
    Args:
        batch_indices: A batched indices generated by sampler
        world_size: number of process
    Returns:

    """
    mini_batchsize = len(batch_indices) // world_size
    reorder_indices = []
    for i in range(0, mini_batchsize):
        for j in range(0, world_size):
            reorder_indices.append(batch_indices[i + j * mini_batchsize])
    return reorder_indices


class RandomIdentitySampler(Sampler):
    """
    Randomly sample N identities, then for each identity,
    randomly sample K instances, therefore batch size is N*K.
    Args:
    - data_source (list): list of (img_path, pid, camid).
    - num_instances (int): number of instances per identity in a batch.
    - batch_size (int): number of examples in a batch.
    """

    def __init__(self, data_source, batch_size, num_instances):
        self.data_source = data_source
        self.batch_size = batch_size
        self.num_instances = num_instances
        self.num_pids_per_batch = self.batch_size // self.num_instances
        self.index_dic = defaultdict(list)  # dict with list value
        # {783: [0, 5, 116, 876, 1554, 2041],...,}
        for index, (_, pid, _, _, _, _) in enumerate(self.data_source):
            self.index_dic[pid].append(index)
        self.pids = list(self.index_dic.keys())

        # estimate number of examples in an epoch
        self.length = 0
        for pid in self.pids:
            idxs = self.index_dic[pid]
            num = len(idxs)
            if num < self.num_instances:
                num = self.num_instances
            self.length += num - num % self.num_instances

    def __iter__(self):
        batch_idxs_dict = defaultdict(list)

        for pid in self.pids:
            idxs = copy.deepcopy(self.index_dic[pid])
            if len(idxs) < self.num_instances:
                idxs = np.random.choice(idxs, size=self.num_instances, replace=True)
            random.shuffle(idxs)
            batch_idxs = []
            for idx in idxs:
                batch_idxs.append(idx)
                if len(batch_idxs) == self.num_instances:
                    batch_idxs_dict[pid].append(batch_idxs)
                    batch_idxs = []

        avai_pids = copy.deepcopy(self.pids)
        final_idxs = []

        while len(avai_pids) >= self.num_pids_per_batch:
            selected_pids = random.sample(avai_pids, self.num_pids_per_batch)
            for pid in selected_pids:
                batch_idxs = batch_idxs_dict[pid].pop(0)
                final_idxs.extend(batch_idxs)
                if len(batch_idxs_dict[pid]) == 0:
                    avai_pids.remove(pid)

        return iter(final_idxs)

    def __len__(self):
        return self.length


class RandomIdentitySampler_AG(Sampler):
    """
    divide the batch into two parts, one for each view
    Randomly sample N identities, then for each identity,
    randomly sample K / 2 instances, therefore batch size is N*K.
    Args:
    - data_source (list): list of (img_path, pid, camid).
    - num_instances (int): number of instances per identity in a batch.
    - batch_size (int): number of examples in a batch.
    """

    def __init__(self, data_source, batch_size, num_instances):
        self.data_source = data_source
        self.batch_size = batch_size
        self.num_instances = num_instances
        self.num_pids_per_batch = self.batch_size // self.num_instances
        self.pid_viewid_index = defaultdict(lambda: defaultdict(list))  # dict with list value
        # {783: {0:[0, 5, 116, 876, 1554, 2041],1:[1,4,797]},...,}
        self.viewids = []
        for index, (_, pid, _, viewid, time_id, _) in enumerate(self.data_source):
            self.pid_viewid_index[pid][viewid].append(index)
            self.viewids.append(viewid)
        self.pids = list(self.pid_viewid_index.keys())
        self.viewids = set(self.viewids)

        self.pid_viewid_index = self.align_nums_between_views(self.pid_viewid_index)
        # length is the int times of num_instances
        self.length = self.get_length(self.pid_viewid_index, self.num_instances // len(self.viewids))

    def __len__(self):
        return self.length

    def __iter__(self):
        sample_views_idxs = self.sample_views(self.pid_viewid_index, self.num_instances // len(self.viewids))
        sample_grouped_idxs_aerial = sample_views_idxs[0]
        sample_grouped_idxs_ground = sample_views_idxs[1]

        final_idxs = []
        avai_pids = copy.deepcopy(self.pids)
        while len(avai_pids) >= self.num_pids_per_batch:
            selected_pids = random.sample(avai_pids, self.num_pids_per_batch)
            for pid in selected_pids:
                batch_idxs_aerial = sample_grouped_idxs_aerial[pid].pop(0)    # 选出一组pid的样本（index）
                batch_idxs_ground = sample_grouped_idxs_ground[pid].pop(0)    # 选出一组pid的样本（index）
                final_idxs.extend(batch_idxs_ground)
                final_idxs.extend(batch_idxs_aerial)
                # aerial_idxs.extend(batch_idxs_aerial)
                # ground_idxs.extend(batch_idxs_ground)
                if len(sample_grouped_idxs_aerial[pid]) == 0 or len(sample_grouped_idxs_ground[pid]) == 0:
                    avai_pids.remove(pid)
        # final_idxs = [*aerial_idxs, *ground_idxs]
        return iter(final_idxs)

    def align_nums_between_views(self, samples_pid_group):
        for pid in self.pids:
            viewids = list(samples_pid_group[pid].keys())
            viewid_imgs = [len(samples_pid_group[pid][viewid]) for viewid in viewids]
            viewid_maxnum_idx = np.argmax(viewid_imgs)
            viewid_maxnum = viewid_imgs[viewid_maxnum_idx]
            for i, viewid in enumerate(viewids):
                if i == viewid_maxnum_idx:
                    continue
                new_pid_view_idx = np.random.choice(samples_pid_group[pid][viewid], size=viewid_maxnum, replace=True)
                samples_pid_group[pid][viewid] = new_pid_view_idx
        return samples_pid_group

    def sample_views(self, samples_pid_group, num_instances_per_view):
        batch_idxs_dict = defaultdict(lambda: defaultdict(list))
        pids = list(samples_pid_group.keys())
        for pid in pids:
            viewids = samples_pid_group[pid].keys()
            for viewid in viewids:
                idxs = copy.deepcopy(samples_pid_group[pid][viewid])
                # 如果索引数少于num_instances_per_view，则进行有放回的随机抽样，直到达到num_instances_per_view
                if len(idxs) < num_instances_per_view:
                    idxs = np.random.choice(idxs, size=num_instances_per_view, replace=True)
                random.shuffle(idxs)
                batch_idxs = []
                for idx in idxs:
                    batch_idxs.append(idx)
                    if len(batch_idxs) == num_instances_per_view:
                        batch_idxs_dict[viewid][pid].append(batch_idxs)
                        batch_idxs = []

        return batch_idxs_dict

    def get_length(self, samples_pid_group, num_instances_per_view):
        length_per_view = 0
        pids = list(samples_pid_group.keys())
        for pid in pids:
            idxs = samples_pid_group[pid][0]
            num = len(idxs)
            if num < num_instances_per_view:
                num = num_instances_per_view
            length_per_view += num - num % num_instances_per_view
        length = length_per_view * len(self.viewids)

        return length

    def get_length2(self, samples_pid_group, num_instances_per_view):
        length_per_view = 0
        pids = list(samples_pid_group.keys())
        for pid in pids:
            for viewid in samples_pid_group[pid].keys():
                idxs = samples_pid_group[pid][viewid]
                num = len(idxs)
                if num < num_instances_per_view:
                    num = num_instances_per_view
                length_per_view += num - num % num_instances_per_view
        length = length_per_view
        return length


class NaiveIdentitySampler(Sampler):
    """
    Randomly sample N identities, then for each identity,
    randomly sample K instances, therefore batch size is N*K.
    Args:
    - data_source (list): list of (img_path, pid, camid).
    - num_instances (int): number of instances per identity in a batch.
    - batch_size (int): number of examples in a batch.
    """

    def __init__(self, data_source: str, mini_batch_size: int, num_instances: int, seed: Optional[int] = None):
        self.data_source = data_source
        self.num_instances = num_instances
        self.num_pids_per_batch = mini_batch_size // self.num_instances

        self._rank = comm.get_rank()
        self._world_size = comm.get_world_size()
        self.batch_size = mini_batch_size * self._world_size

        self.pid_index = defaultdict(list)

        for index, info in enumerate(data_source):
            pid = info[1]
            self.pid_index[pid].append(index)

        self.pids = sorted(list(self.pid_index.keys()))
        self.num_identities = len(self.pids)

        if seed is None:
            seed = comm.shared_random_seed()
        self._seed = int(seed)

    def __iter__(self):
        start = self._rank
        yield from itertools.islice(self._infinite_indices(), start, None, self._world_size)

    def _infinite_indices(self):
        # np.random.seed(self._seed)
        while True:
            avl_pids = copy.deepcopy(self.pids)
            batch_idxs_dict = {}

            batch_indices = []
            while len(avl_pids) >= self.num_pids_per_batch:
                selected_pids = np.random.choice(avl_pids, self.num_pids_per_batch, replace=False).tolist()
                for pid in selected_pids:
                    # Register pid in batch_idxs_dict if not
                    if pid not in batch_idxs_dict:
                        idxs = copy.deepcopy(self.pid_index[pid])
                        if len(idxs) < self.num_instances:
                            idxs = np.random.choice(idxs, size=self.num_instances, replace=True).tolist()
                        np.random.shuffle(idxs)
                        batch_idxs_dict[pid] = idxs

                    avl_idxs = batch_idxs_dict[pid]
                    for _ in range(self.num_instances):
                        batch_indices.append(avl_idxs.pop(0))

                    if len(avl_idxs) < self.num_instances:
                        avl_pids.remove(pid)

                if len(batch_indices) == self.batch_size:
                    yield from reorder_index(batch_indices, self._world_size)
                    batch_indices = []

    def __len__(self):
        # estimate number of examples in an epoch
        # just drop the remaining parts
        identities = self.pids
        drop_indices = self.num_identities % (self.num_pids_per_batch * self._world_size)
        if drop_indices:
            identities = identities[:-drop_indices]

        length = 0
        for pid in identities:
            items_pid = self.pid_index[pid]
            num_items = len(items_pid)
            if num_items < self.num_instances:
                num_items = self.num_instances
            last_items = num_items % self.num_instances
            length += num_items + self.num_instances - last_items

        return length


class ViewBalancedSampler_xhao(Sampler):
    """
    divide the batch into two parts, one for each view
    Randomly sample N identities, then for each identity,
    randomly sample K / 2 instances, therefore batch size is N*K.
    Args:
    - data_source (list): list of (img_path, pid, camid).
    - num_instances (int): number of instances per identity in a batch.
    - batch_size (int): number of examples in a batch.
    """

    def __init__(self, data_source: List, mini_batch_size: int, num_instances: int, seed: Optional[int] = None):
        self.data_source = data_source
        self.num_instances = num_instances
        self.num_pids_per_batch = mini_batch_size // self.num_instances

        self._rank = comm.get_rank()
        self._world_size = comm.get_world_size()
        self.batch_size = mini_batch_size * self._world_size

        self.index_pid = dict()
        self.pid_cam = defaultdict(list)
        self.pid_view = defaultdict(list)
        self.pid_index = defaultdict(list)
        self.pid_time = defaultdict(list)

        for index, info in enumerate(data_source):
            pid = info[1]
            camid = info[2]
            viewid = info[3]
            self.index_pid[index] = pid
            self.pid_cam[pid].append(camid)
            self.pid_view[pid].append(viewid)
            self.pid_index[pid].append(index)

        self.pids = sorted(list(self.pid_index.keys()))
        self.num_identities = len(self.pids)

        if seed is None:
            seed = comm.shared_random_seed()
        self._seed = int(seed)

        self._rank = comm.get_rank()
        self._world_size = comm.get_world_size()

    def __iter__(self):
        start = self._rank
        yield from itertools.islice(self._infinite_indices(), start, None, self._world_size)

    def _infinite_indices(self):
        np.random.seed(self._seed)
        while True:
            # Shuffle identity list
            identities = np.random.permutation(self.num_identities)

            # If remaining identities cannot be enough for a batch,
            # just drop the remaining parts
            drop_indices = self.num_identities % (self.num_pids_per_batch * self._world_size)
            if drop_indices:
                identities = identities[:-drop_indices]

            batch_indices = []
            for kid in identities:
                i = np.random.choice(self.pid_index[self.pids[kid]])
                _, i_pid, i_cam, i_view, i_time = self.data_source[i]
                # batch_indices.append(i)
                pid_i = self.index_pid[i]
                cams = self.pid_cam[pid_i]
                views = self.pid_view[pid_i]
                index = self.pid_index[pid_i]
                # select_cams = no_index(cams, i_cam)
                # select_diff_cams = no_index(cams, i_cam)
                # select_same_cams = same_index(cams, i_cam)

                select_same_cams, select_diff_cams = view_divide(views, i_view)
                assert len(select_diff_cams) + len(select_same_cams) == len(index)

                if select_diff_cams:
                    if len(select_diff_cams) >= self.num_instances // 2:
                        cam_indexes = np.random.choice(select_diff_cams, size=self.num_instances // 2, replace=False)
                    else:
                        cam_indexes = np.random.choice(select_diff_cams, size=self.num_instances // 2, replace=True)
                    for kk in cam_indexes:
                        batch_indices.append(index[kk])

                    if len(select_same_cams) >= self.num_instances // 2:
                        cam_indexes = np.random.choice(select_same_cams, size=self.num_instances // 2, replace=False)
                    else:
                        cam_indexes = np.random.choice(select_same_cams, size=self.num_instances // 2, replace=True)
                    for kk in cam_indexes:
                        batch_indices.append(index[kk])
                else:
                    if len(select_same_cams) == 1:
                        # Only one image for this identity
                        ind_indexes = [0] * self.num_instances
                    elif len(select_same_cams) >= self.num_instances:
                        ind_indexes = np.random.choice(select_same_cams, size=self.num_instances, replace=False)
                    else:
                        ind_indexes = np.random.choice(select_same_cams, size=self.num_instances, replace=True)

                    for kk in ind_indexes:
                        batch_indices.append(index[kk])

                if len(batch_indices) == self.batch_size:
                    yield from reorder_index(batch_indices, self._world_size)
                    batch_indices = []

# [3066, 3067, 8004, 8005, 11613, 11614, 15017, 18437, 18438, 32867, 32868, 36645, 42396, 46228]
